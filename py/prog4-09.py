# -*- coding: utf-8 -*-
"""prog4-09.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Q8oadkQk3V4jxRHtpF2W6tOfWg2zMIUn
"""

from google.colab import files
files.upload() # kaggle.jsonをアップロード
!mkdir -p ~/.kaggle
!mv kaggle.json ~/.kaggle/
!chmod 600 /root/.kaggle/kaggle.json

import pandas as pd
import numpy as np
from sklearn.linear_model import LogisticRegression
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split
from sklearn.impute import SimpleImputer
# LDAを利用するためにインポート
from sklearn.discriminant_analysis \
 import  LinearDiscriminantAnalysis
from sklearn.metrics import accuracy_score,\
 classification_report

def prepare():
    !kaggle datasets download -d \
    theoverman/the-spotify-hit-predictor-dataset
    !unzip the-spotify-hit-predictor-dataset.zip

    data, labs = [], []
    years = ['60', '70', '80', '90', '00', '10']
    for yi, year in enumerate(years):
        df = pd.read_csv('./dataset-of-%ss.csv' % year )
        if len(data) == 0:
            data = df
        else:
            data = pd.concat( [data, df] )
    print(len(data))
    features = []
    for f in data.columns.values[:-1]:
        if not f in ['track', 'artist', 'uri', 'target']: 
            features.append(f)

    X_train = data.loc[:, features].values
    y_train = data.loc[:, ['target']].values.ravel()
    print(len(X_train), len(y_train))
    return X_train, y_train, years

# 欠損値の補完、標準化
def preprocess(x):
    # 単一代入法
    simple_imp = SimpleImputer(missing_values=np.nan,\
                strategy='mean')
    simple_imp.fit( x )
    x_imp = simple_imp.transform(x)
    sc = StandardScaler()
    x_std = sc.fit_transform(x_imp)
    return x_std

def main():
    X_train, y_train, years = prepare()
    labels = ['hit', 'flop']
    n_class = len(labels)
    X_train_std = preprocess(X_train)
    X_train_std, X_test_std, y_train, y_test =\
    train_test_split(X_train_std, y_train, \
                    train_size=0.7, random_state=1)
    print('次元削減無し, ロジスティック回帰')
    lr = LogisticRegression()
    lr.fit(X_train_std, y_train)
    y_pred = lr.predict(X_test_std)
    print('Accuracy: %.2f' % accuracy_score(y_test,y_pred))
    print(classification_report(y_test, y_pred,\
                           target_names=labels))
    print('\n--LDAで次元削減(次元数:%d)および予測--' % (\
                                  n_class-1))

    # LDAでは、次元削減できる次元数の上限は、
    # "クラス数-1" となる。
    lda = LinearDiscriminantAnalysis(n_components=n_class-1)
    X_train_lda = lda.fit_transform(X_train_std, y_train)
    print(X_train_lda[0])
    X_test_lda = lda.transform(X_test_std)
    lr = LogisticRegression()
    lr.fit(X_train_lda, y_train)
    y_pred = lr.predict(X_test_lda)
    print('Accuracy: %.2f' % accuracy_score(y_test,y_pred))
    print(classification_report(y_test, y_pred, \
                           target_names=labels))
if __name__ == '__main__':
    main()